{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11174474,"sourceType":"datasetVersion","datasetId":6974151}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_bert_embeddings(urls):\n    \"\"\"\n    Extract embeddings from URLs using BERT.\n    \"\"\"\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertModel.from_pretrained('bert-base-uncased').to('cuda')\n    # model = BertModel.from_pretrained('bert-base-uncased')\n    model.eval()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    url_embeddings = []\n    print(\"Extracting BERT Embeddings...\")\n    for url in tqdm(urls, desc=\"Embedding URLs\"):\n        inputs = tokenizer(url, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n        # inputs = tokenizer(url, return_tensors='pt', padding=True, truncation=True, max_length=128)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n\n        # embeddings = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n        url_embeddings.append(embeddings)\n\n    return url_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Load Dataset\ndata = pd.read_csv('/kaggle/input/info-sec-a2-data/balanced_data_with_features.csv')\n\n# Step 2: Train-Test Split\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Step 3: Extract URL Embeddings for Train and Test Sets\ntrain_urls = train_data['url'].tolist()\ntest_urls = test_data['url'].tolist()\n\ntrain_embeddings = extract_bert_embeddings(train_urls)\ntest_embeddings = extract_bert_embeddings(test_urls)\n\n# Step 4: Prepare Labels\ny_train = train_data['label'].values\ny_test = test_data['label'].values\n\n# Step 5: Train a Simple Classifier (Logistic Regression)\nprint(\"Training the Classifier...\")\nclassifier = LogisticRegression(max_iter=1000)\nclassifier.fit(train_embeddings, y_train)\n\n# Step 6: Make Predictions and Evaluate Model\nprint(\"Evaluating the Model...\")\ny_pred = classifier.predict(test_embeddings)\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hello\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# trying another classifier","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:09:14.653698Z","iopub.execute_input":"2025-03-27T11:09:14.653999Z","iopub.status.idle":"2025-03-27T11:09:14.658937Z","shell.execute_reply.started":"2025-03-27T11:09:14.653977Z","shell.execute_reply":"2025-03-27T11:09:14.657636Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def extract_bert_embeddings(urls):\n    \"\"\"\n    Extract embeddings from URLs using BERT with GPU support (if available).\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertModel.from_pretrained('bert-base-uncased').to(device)\n    model.eval()\n\n    url_embeddings = []\n    print(\"Extracting BERT Embeddings...\")\n    for url in tqdm(urls, desc=\"Embedding URLs\"):\n        inputs = tokenizer(url, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n        url_embeddings.append(embeddings)\n\n    return url_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:09:15.544819Z","iopub.execute_input":"2025-03-27T11:09:15.545098Z","iopub.status.idle":"2025-03-27T11:09:15.550972Z","shell.execute_reply.started":"2025-03-27T11:09:15.545079Z","shell.execute_reply":"2025-03-27T11:09:15.549905Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import joblib\n\n\ndef train_and_save_model(model, model_name, train_embeddings, y_train, test_embeddings, y_test):\n    \"\"\"\n    Train a given model, print classification report, and save the model weights.\n    \"\"\"\n    print(f\"\\nTraining {model_name}...\")\n    model.fit(train_embeddings, y_train)\n    print(f\"Evaluating {model_name}...\")\n    y_pred = model.predict(test_embeddings)\n    report = classification_report(y_test, y_pred)\n    print(report)\n\n    # Save the trained model\n    model_path = f\"{model_name}_model.pkl\"\n    joblib.dump(model, model_path)\n    print(f\"{model_name} model saved successfully at {model_path}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:09:16.617954Z","iopub.execute_input":"2025-03-27T11:09:16.618235Z","iopub.status.idle":"2025-03-27T11:09:16.623336Z","shell.execute_reply.started":"2025-03-27T11:09:16.618214Z","shell.execute_reply":"2025-03-27T11:09:16.622350Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Step 1: Load Dataset\ndata = pd.read_csv('/kaggle/input/info-sec-a2-data/balanced_data_with_features.csv')\n\n# Step 2: Train-Test Split\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Step 3: Extract URL Embeddings for Train and Test Sets\ntrain_urls = train_data['url'].tolist()\ntest_urls = test_data['url'].tolist()\n\ntrain_embeddings = extract_bert_embeddings(train_urls)\ntest_embeddings = extract_bert_embeddings(test_urls)\n\n# Step 4: Prepare Labels\ny_train = train_data['label'].values\ny_test = test_data['label'].values\n\n# Step 5: Train and Save Models Individually\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\ntrain_and_save_model(rf_classifier, 'RandomForest', train_embeddings, y_train, test_embeddings, y_test)\n\nxgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\ntrain_and_save_model(xgb_classifier, 'XGBoost', train_embeddings, y_train, test_embeddings, y_test)\n\nsvm_classifier = SVC(kernel='rbf', probability=True)\ntrain_and_save_model(svm_classifier, 'SVM', train_embeddings, y_train, test_embeddings, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:09:19.776431Z","iopub.execute_input":"2025-03-27T11:09:19.776777Z"}},"outputs":[{"name":"stdout","text":"Extracting BERT Embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Embedding URLs: 100%|██████████| 411972/411972 [55:26<00:00, 123.85it/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting BERT Embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Embedding URLs: 100%|██████████| 102994/102994 [13:52<00:00, 123.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTraining RandomForest...\nEvaluating RandomForest...\n              precision    recall  f1-score   support\n\n           0       0.91      0.89      0.90     18778\n           1       0.92      0.95      0.94     20048\n           2       0.95      0.98      0.96     18912\n           3       0.58      0.61      0.59     23738\n           4       0.56      0.51      0.53     21518\n\n    accuracy                           0.77    102994\n   macro avg       0.78      0.79      0.78    102994\nweighted avg       0.77      0.77      0.77    102994\n\nRandomForest model saved successfully at RandomForest_model.pkl.\n\nTraining XGBoost...\nEvaluating XGBoost...\n              precision    recall  f1-score   support\n\n           0       0.92      0.92      0.92     18778\n           1       0.95      0.95      0.95     20048\n           2       0.97      0.99      0.98     18912\n           3       0.59      0.58      0.58     23738\n           4       0.55      0.56      0.56     21518\n\n    accuracy                           0.78    102994\n   macro avg       0.80      0.80      0.80    102994\nweighted avg       0.78      0.78      0.78    102994\n\nXGBoost model saved successfully at XGBoost_model.pkl.\n\nTraining SVM...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}